% \documentclass{article}
\documentclass[14pt, a4paper]{extarticle} 
% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english, russian]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size

\usepackage[utf8]{inputenc} 
\usepackage[T2A]{fontenc}
\usepackage[a4paper,top=2cm,bottom=2cm,left=3cm, right=15mm,marginparwidth=1.75cm]{geometry}
\usepackage{indentfirst}
\usepackage{csquotes}
\linespread{1.5}
\parindent=1.25cm

\usepackage{amsmath}
\usepackage[]{graphicx}
\usepackage{float}
\usepackage[backend=biber, 
    style=gost-numeric, 
    language=auto, 
    autolang=other, 
    sorting=none]{biblatex}

\addbibresource{references.bib}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\usepackage{fancyhdr} 
\pagestyle{fancy} 
\fancyhf{}
\fancyfoot[C]{\thepage} 
\pagenumbering{arabic}
\renewcommand{\headrulewidth}{0pt} 
\AtBeginEnvironment{tabular}{\small}

\title{Профсеминар}
\author{Глеб Чекмарев}

\begin{document}
  \begin{titlepage} % Suppresses displaying the page number on the title page and the subsequent page counts as page 1
\fontsize{14pt}{14pt}\selectfont
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for horizontal lines, change thickness here
 
\center % Centre everything on the page
 
%------------------------------------------------
%   Headings
%------------------------------------------------
 
\textsc{ФЕДЕРАЛЬНОЕ  ГОСУДАРСТВЕННОЕ АВТОНОМНОЕ}\\
\textsc{ОБРАЗОВАТЕЛЬНОЕ УЧРЕЖДЕНИЕ ВЫСШЕГО ОБРАЗОВАНИЯ}\\
\textsc{«НАЦИОНАЛЬНЫЙ ИССЛЕДОВАТЕЛЬСКИЙ УНИВЕРСИТЕТ}\\
\textsc{«ВЫСШАЯ ШКОЛА ЭКОНОМИКИ»}\\
\textsc{\bfseries Московский институт электроники и математики}\\[1.5cm]
 
\textsc{Чекмарев Глеб Вениаминович}\\
\textsc{\large\bfseries Градиентный спуск} % Major heading such as course name
 
 
% If you don't want a supervisor, uncomment the two lines below and comment the code above
\vfill\vfill
\textsc{Отчёт по дисциплине}
\textsc{"Профориентационный семинар "Введение в специальность"}\\
 
%------------------------------------------------
%   Date
%------------------------------------------------
 
\begin{flushright}
Студент\\
Г.В. Чекмарев
\end{flushright}
% % \hfil Студент   
% \namesigdate[5cm]{Подпись} 
 
\hfill
\begin{minipage}{0.45\textwidth}
    \begin{tabular}{p{\textwidth}}
    \begin{flushright}
    Руководитель\\
    Д.Д. Суховерхова\\[0.5cm]
    \end{flushright}
    \end{tabular}
\end{minipage}% 
 
\vfill\vfill\vfill\vfill % Position the date 3/4 down the remaining page
 
{\large\bfseriesМосква \the\year г.} % Date, change the \today to a set date if you want to be precise
 
%------------------------------------------------
%   Logo
%------------------------------------------------
 
% \vfill\vfill
% \includegraphics[width=0.2\textwidth]{frog.jpg}\\[1cm] % Include a department/university logo - this will require the graphicx package
 
%----------------------------------------------------------------------------------------
 
%    \vfill % Push the date up 1/4 of the remaining page
\end{titlepage}
\newpage
\section{Аннотация}
Цель работы —  решение задачи регресси с испльзованием оптимизации функции потерь линейной модели с помощью метода градиентного спуска, реализованного на python.

Метод градиентного спуска\cite{gd} является основополагающим в задачах численной оптимизаций моделей машинного обучения, поэтому крайне важно понимать природу задачи, и в чём заключаются тонкости его использования.

Для оценки эффективности алгоритма проведено сравнение с методами LinearRegression и Lasso машинного обучения из библиотеки scikit-learn, что подтвердило корректность разработанного решнения. 

В ходе работы было реализована усовершенствованная версия алгоритма градиентного спуска, использующая метод Нестерова \cite{nag} для увеличения скорости обучения и повышения точности поиска оптимума.

Также был проведён разведочный анализ (eda) для выявления наиболее значимых фрагментов датасета и улучшения результатов модели.

Работа демонстрирует, что в конечном результате важна не только корректность реализации каждого из промежуточных этапов, но и их взаимодействие.
\newpage
\tableofcontents
\newpage
\section{Введение}
При решении задача регрессии и классификации используются различные методы машинного обучения, заключающиеся в создании некоторой функции, которая сможет с наибольшей точностью делать предсказания на данных, с которыми раньше не сталкивался. Для обучения таких моделей собирают датасеты, которые наиболее полно отражают зависимости между аргументами, процесс обучения заключается в том, чтобы найти параметры этой функции, при которых ошибка будет наименьшей. Математическая интерпретация — поиск точки $w$, координаты которой — параметры модели, в которой функция потерь $L(w)$ достигает минимума, в большинстве случаев $L(w)$ — гладкая, поэтому задача оптимизации сводится к поиску наилучшего локального минимума, для выпуклых функций задача упрощается, так как у выпуклых функций только один локальный минимум, причём он совпадает с глобальным минимумом.

Наиболее популярный метод, применяемый для решения описанной задачи — градиентный спуск из-за его относительной простоты, скорости работы и точности.

Как известно для поиска локального минимума функции многих переменных можно использовать аналитические методы, например, подсчёт гессиана в точке. Такой метод неэффективен из-за необходимости подсчёта большого количества производных, выполняемого не менее чем за $O(n^2)$, где $n$ — количество параметров модели\footnote{Необходимо найти симметричную матрицу $n \times n$}, и последующей проверки критической точки через вычисление угловых миноров гессиана за $O(n^{3.81})$\footnote{
\href{https://stanford.edu/~rezab/classes/cme323/S16/notes/Lecture03/cme323_lec3.pdf}{Алгоритм Штрассена} для перемножения матриц за
$O(n ^{log_2 7})$}, что недопустимо сложная операция для моделей, число параметров которых может заметно превосходить сотни тысяч.

По аналогичной причине не практичен и метод наименьших квадратов, позволяющий аналитически вычислить точку минимума линейной модели $\tilde y = xw$, где $w$ — набор весов, $x$ — аргумент модели,$\tilde y$ — предсказание:
$$A^TAw = A^Ty,$$
$$w = (A^T A)^{-1}A^T y$$
Также следует отметить, что у матрицы $A^TA$ может не быть обратной

В ходе работы над практическими задачам зачастую приходится дообучать модели на новых данных, что невозможно для вышеупомянутых методов без полного переподсчёта, однако легко реализуется повторным запуском градиентного спуска без обнуления текущих параметров модели.

К тому же аналитическая оптимизация почти не применима к некоторым функциям из-за сложности их вычисления и/или исследования. Например компактная запись функции средней нейронной сети невозможна из-за композиции огромного числа других, более простых функций. А некоторые функции потерь вовсе неэлементарны: Wasserstein Loss, Policy Gradient Loss, LovaszLoss.

Градиентный спуск не имеет этих недостатков, поэтому занимает лидирующую позицию в современном машинном обучении.
\newpage
\section{Постановка задачи}
Решить задачу регрессии с использованием оптимизации градиентным спуском. Целевая функция - MSE (среднеквадратичное отклонение):
$$MSE(\textbf{y}, \tilde{\textbf{y}}) = \frac{1}{N}\sum_{i=1}^N(y_i - \tilde{y_i})^2$$
где \textbf{y} - вектор истинных значений, $\tilde{\textbf{y}}$- вектор предсказаний, $\tilde{\textbf{y}} = a_0 + a_1x_1^i + ... +a_nx_n^i$ — линейная комбинация признаков $x_j^i$

Использовать датасет \href{https://www.kaggle.com/datasets/mirichoi0218/insurance}{Medical Cost Personal Datasets} для обучения моделей. Он содержит 1338 строк информации о людях в США и их стоимости медицинской страховки.

Столбцы датасета:
\begin{enumerate}
\item age: (число), возраст человека
\item sex: ('female', 'male'), пол человека
\item bmi: (число), индекс массы тела человека
\item children: (число), количества детей у человека
\item smoker: ('yes', 'no'), факт курения человека
\item region: ('northeast', 'southeast', 'southwest', 'northwest'), регион США, в котором проживает человек
\item charges: (число), стоимость медицинской страховки
\end{enumerate}
\newpage
\section{Методы}
\subsection{Градиентный спуск}
Градиентный спуск заключается в постепенном перемещении точки в направлении противоположном производной в этой точке.
$$w_{i+1} = w_i - \eta \nabla L(w_i)$$
где $w_k$ — точка на $k$-ом шаге алгоритма, $L$ — функция потерь, $\eta$ — коэффициент скорости обучения("время" которое перещаемся со "скоростью" $\nabla L(w_i)$)
\begin{figure}[ht]
  \centering
  \begin{minipage}[c]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{GDMultiDimensionalExample.png}
  \end{minipage}
  \hfill
  \begin{minipage}[c]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{GDIllustration.png}
  \end{minipage}
  \caption{Работа градиентного спуска}
  \label{fig:GDExample}
\end{figure}
\subsection{Метод Нестерова (NAG)}
Метод Нестерова \cite{nag} ускоряет сходимость алгоритма градиентного спуска и позволяет лучше обходить мелкие локальные минимумы за счёт вычисления градиента в промежуточных точках и добавления корректирующего члена
$$\phi_{t+1} = w_t - \eta \nabla L(w_t)$$
$$w_{t+1}=\phi_{t+1} + \mu(\phi_{t+1} - \phi_t)$$
где $\phi_{t+1}$ — промежуточная точка итерации, от которой будет отталкиваться расчёт последующих параметров модели $w_{t+1}$, $\eta$ — скорость обучения(learning rate), $\mu$ — коэффициент импульса

В современных приложениях чаще используют вариант Ильи Суцкевера, так как он проще в реализации.
$$w_{t+1} = \phi_t + \mu(\phi_t - \phi_{t-1})$$
$$\phi_{t+1} = w_{t+1} - \eta \nabla L(w_{t+1})$$
$$\Updownarrow$$
$$v_t:= \phi_t - \phi_{t-1}$$
$$w_{t+1} = \phi_t + \mu v_t$$
$$\phi_{t+1}= \phi_t + \mu v_t - \eta \nabla L(\phi_t +\mu v_t)$$

\subsection{Полиномиальные признаки}
Не всегда целевая переменная зависит от признаков линейно, в таком случае линейные модели не применимы. Но существуют способы модификации данных, чтобы получить более ровные зависимости. Один из них — добавление дополнительных признаков, являющихся произведениями уже имеющихся, так, к признакам $$a,b,c$$ при использовании полиномиальных признаков степени 2, добавляются
$$ab, ac, bc, a^2, b^2,c^2$$
Что позволит создать линейную модель в некотором смысле учитывающую сложные связи.  
\newpage
\section{Результаты}
\subsection{EDA}
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{correlations.png}
  \caption{Корелляции между данными}
  \label{fig:correlations}
\end{figure}

Из Рисунка~\ref{fig:correlations} видно, что на charges наиболее сильно влияет, курит человек или нет, bmi и age влияют, но не столь сильно, влияение прочих признаков отстутствует.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{smokersCharges.png}
  \caption{Разброс цен}
  \label{fig:smokersCharges}
\end{figure}

Действительно, медианное значение charges у курильщиков(34456) почти в 5 раз выше, чем у не курильщиков(7345), а разброс цены на страховку значительно выше (см. Рис.~\ref{fig:smokersCharges}).

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{smokersVizualization2.png}
  \caption{Линейный зависимости между признаками}
  \label{fig:smokersVizualization2}
\end{figure}

Из Рисунка~\ref{fig:smokersVizualization2} понятно, что на стоимость страховки у курильщиков влияет в большей степени индекс массы тела(имт), чем возраст, а у некурящих — возраст

Необходимо учесть эти зависимости в данных при подготовке данных для обучения линейной модели, для этого будем использовать полиномиальные признаки. 

Так как на charges влияют только факт курения, возраст и имт, поэтому не будем учитывать прочие признаки при обучении модели
\subsection{Сравнение результатов градентного спуска с другими методами}
Из-за простоты данных,в которых присуствуют отчётливые линейный зависимости, и выпуклой функции потерь\footnote{MSE при вычислении от параметров линейной модели — выпуклая}, не удалось достичь ощутимых различий между NAG и классическим подходом без импульса

Для оценки эффективности алгоритмов используем не только MSE, но и метрику $\text{R}^2$(Коэффициент Детерминации),так как она более наглядная. Это метрика, в некотором смысле показывающая, какую долю разброса модель может объяснить
$$\text{R}^2 = 1 - \frac{\sum_{i}(y_i - \tilde   y_i)}{\sum _i(y_i - \bar y)} \le 1$$где $\tilde   y_i$ — предсказанное значение, $\bar y$ — среднее арифметическое $y_i \quad i=1,2,3,..,n$, чем ближе к единице, тем лучше модель справляется с предсказаниями.

Во всех испытания оба градиентных спуска выполнялись до итерации, на котором абсолютное изменение MSE становилось меньше $10^{-4}$

В Таблице~\ref{tab:NAG_VS_CLASSIC} NAG выполнялся с коэффициентом импульса $\mu  = 0.95$:

\begin{table}[H]
  \centering
  \begin{tabular}{|c|c|c|c|c|}
    \hline
    \multirow{learning rate} & \multicolumn{2}{c|}{NAG} & \multicolumn{2}{c|}{Классический град. спуск} \\
    \cline{2-5}
    & Последний шаг & Время, с & Последний шаг & Время, с \\
    \hline
    $10^{-1}$ & 4971 & 2.032 & 4971 & 2.413 \\
    \hline
    $10^{-3}$ & 351563 & 168.569 & 351561 & 153.86 \\
    \hline
  \end{tabular}
  \caption{Сравнение скорости сходимости}
  \label{tab:NAG_VS_CLASSIC} 
\end{table}

Изменение коэффициента сохранение импульса не привели к изменению эффективности, запуски в Таблице~\ref{tab:NAG_Impulse} произведены при $\text{learning rate} = 10^{-3}$
\begin{table}[H]
  \centering
  \begin{tabular}{|c | c |}
        \hline
      Коэффициент сохранения импульса $\mu$ & Последняя итерация NAG \\
       \hline
       0.90 & 351563 \\
       \hline
       0,95 & 351563 \\
       \hline
       0.99& 351563\\
        \hline
\end{tabular}
\caption{Зависимость скорости сходимости NAG от $\mu$}
\label{tab:NAG_Impulse} 
\end{table}

В Таблице~\ref{tab:battle} импульс в NAG (0.95), learning rate =$10^{-3}$, реализации МНК(LinearRegression) и Lasso взяты из библиотеки scikit-learn
\begin{table}[H]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
    \hline
    \multirow{Признаки} & \multicolumn{2}{c|}{NAG} & \multicolumn{2}{c|}{Классический} & \multicolumn{2}{c|}{МНК} & \multicolumn{2}{c|}{Lasso} \\
    \cline{2-9}
    & MSE & R² & MSE & R² & MSE & R² & MSE & R² \\
    \hline
    обычные & 36149113 & 0.725 & 36149113 & 0.725 & 36149436 & 0.725 & 36148131 & 0.725 \\
    \hline
    полиномиальные & 23682580 & 0.820 & 23682580 & 0.820 & 23682454 & 0.820 & 23678030 & 0.820 \\
    \hline
  \end{tabular}
  \caption{MSE и R² различных методов}
  \label{tab:battle} 
\end{table}

Как можно видеть из таблиц, градиентный спуск ничем не уступает аналитеческому решению с помощью МНК. А результат сопоставим с более продвинутым алгоритмом —Lasso.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{differences.png}
  \caption{Распределение ошибки $y - \tilde y$ }
  \label{fig:differences}
\end{figure}

433 предсказания из 536 отличаются от действительных данных не более чем на 4697, верхний ус — 2502, нижний ус — -4697, стандартное отклонение — 4854 (см. Рис.~\ref{fig:differences}).


\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{scatterDiffers.png}
  \caption{Сопоставление предсказанных и наблюдаемых зависимостей}
  \label{fig:scatterDiffers}
\end{figure}

\begin{figure}[H]
  \centering
    \includegraphics[width=\textwidth]{predictedVizualization2.png}
  \caption{Линейный зависимости между данными}
  \label{fig:predictedVizualization2}
\end{figure}

Линейны зависимости, выявленные моделью также совпали с изначальными (см. Рис.~\ref{fig:predictedVizualization2})

Ноутбук доступен по ссылке \href{https://github.com/glchek/HSE-PROFSEMINAR}{github.com/glchek/HSE-PROFSEMINAR}
\newpage
\section{Заключение}
В ходе работы была успешно решена задача регрессии с использованием метода градиентного спуска и его модификации — алгоритма Нестерова. Реализация подтвердила, что градиентный спуск нисколько не уступает аналитическим методам. Важным шагом стало добавление полиномиальных признаков, что повысило коэффициент детерминации с 0.725 до 0.820.

Метод Нестерова, обеспечивающий ускорение сходимости для выпуклых функций, в данной задаче не показал преимуществ. Это может быть связано с особенностями данных, где классический градиентный спуск и так быстро достигает локального минимума. Тесты показали важность подбора гиперпараметров: при больших значениях коэффициента обучения ($\eta$=0.1) оба метода сошлись за 2–3 секунды, тогда как при $\eta = 10^{-3}$ затраченное время увеличилось до 150 секунд.

Работа демонстрирует, что выбор алгоритма оптимизации должен учитывать свойства целевой функций, а для выпуклых задач простые алгоритмы, с правильно подобранными гиперпараметрами,  сравнимы по эффективности с продвинутыми. 


\newpage
\printbibliography
\addcontentsline{toc}{section}{Список литературы}
\end{document}